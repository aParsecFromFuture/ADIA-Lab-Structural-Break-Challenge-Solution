{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T20:13:01.371480Z",
     "start_time": "2025-09-28T20:13:00.549649Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import crunch\n",
    "\n",
    "crunch = crunch.load_notebook()"
   ],
   "id": "198a685788ac3e13",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded inline runner with module: <module '__main__'>\n",
      "\n",
      "cli version: 7.5.0\n",
      "available ram: 31.71 gb\n",
      "available cpu: 28 core\n",
      "----\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T20:13:06.331251Z",
     "start_time": "2025-09-28T20:13:01.392967Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np # == 2.1.2\n",
    "import pandas as pd # == 2.3.2\n",
    "import polars as pl # == 1.2.1\n",
    "import os\n",
    "import typing\n",
    "import joblib # == 1.5.2\n",
    "import sklearn # == 1.6.1\n",
    "import lightgbm as lgb # == 4.6.0\n",
    "from tabpfn import TabPFNClassifier # == 2.1.3\n",
    "import shap # == 0.48.0\n",
    "import scipy # == 1.16.1\n",
    "\n",
    "from scipy.stats import f, ks_2samp, levene\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, clone"
   ],
   "id": "d12fa1d511952ed",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Farukcan\\PycharmProjects\\ADIA Lab Structural Break Challenge\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T20:13:06.492505Z",
     "start_time": "2025-09-28T20:13:06.488834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FirstFeatureGenerator:\n",
    "    def generate(self, X, suffix):\n",
    "        return (\n",
    "            X\n",
    "            .group_by('id')\n",
    "            .agg(\n",
    "                pl.col('value').mean().alias('val_mean'),\n",
    "                pl.col('value').median().alias('val_median'),\n",
    "                pl.col('value').max().alias('val_max'),\n",
    "                pl.col('value').min().alias('val_min'),\n",
    "                pl.col('value').std().alias('val_std'),\n",
    "                pl.col('value').skew().alias('val_skew'),\n",
    "            )\n",
    "            .with_columns(\n",
    "                (pl.col('val_mean') / pl.col('val_std')).alias('mean_norm'),\n",
    "                (pl.col('val_median') / pl.col('val_std')).alias('median_norm'),\n",
    "            )\n",
    "            .sort('id')\n",
    "            .drop('id')\n",
    "            .rename(lambda col: suffix + col)\n",
    "        )"
   ],
   "id": "2c94783bf28a5a8",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T20:13:06.506917Z",
     "start_time": "2025-09-28T20:13:06.501035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SecondFeatureGenerator:\n",
    "    def generate(self, X, suffix):\n",
    "        def flatten_2d(lst):\n",
    "            return [item for sublist in lst for item in sublist]\n",
    "\n",
    "        return (\n",
    "            X\n",
    "            .with_columns(\n",
    "                pl.col('value').alias('value_1'),\n",
    "                ((pl.col('value') - pl.col('value').mean().over('id')) / pl.col('value').std().over('id')).alias(\n",
    "                    'value_2'),\n",
    "                pl.col('value').cum_sum().over('id').alias('value_3'),\n",
    "                (pl.col('value').rank('dense').over('id') / pl.col('value').count().over('id')).alias('value_4'),\n",
    "                pl.col('value').abs().alias('value_5'),\n",
    "                pl.col('value').rolling_mean(16).over('id').alias('value_6'),\n",
    "                pl.col('value').rolling_std(16).over('id').alias('value_7'),\n",
    "            )\n",
    "            .group_by('id')\n",
    "            .agg(flatten_2d([[\n",
    "                pl.col(f'value_{i}').mean().alias(f'val_mean_{i}'),\n",
    "                pl.col(f'value_{i}').median().alias(f'val_median_{i}'),\n",
    "                pl.col(f'value_{i}').max().alias(f'val_max_{i}'),\n",
    "                pl.col(f'value_{i}').min().alias(f'val_min_{i}'),\n",
    "                pl.col(f'value_{i}').std().alias(f'val_std_{i}'),\n",
    "                pl.col(f'value_{i}').skew().alias(f'val_skew_{i}'),\n",
    "            ] for i in [1, 2, 3, 4, 5, 6, 7]]))\n",
    "            .with_columns(flatten_2d([[\n",
    "                (pl.col(f'val_mean_{i}') / pl.col(f'val_std_{i}')).alias(f'mean_norm_{i}'),\n",
    "                (pl.col(f'val_median_{i}') / pl.col(f'val_std_{i}')).alias(f'median_norm_{i}'),\n",
    "            ] for i in [1, 2, 3, 4, 5, 6, 7]]))\n",
    "            .sort('id')\n",
    "            .drop('id')\n",
    "            .rename(lambda col: suffix + col)\n",
    "        )"
   ],
   "id": "737d4696cedc145c",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T20:13:06.533694Z",
     "start_time": "2025-09-28T20:13:06.517823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ThirdFeatureGenerator:\n",
    "    def generate(self, X, suffix):\n",
    "        def flatten_2d(lst):\n",
    "            return [item for sublist in lst for item in sublist]\n",
    "\n",
    "        X = (\n",
    "            X\n",
    "            .with_columns(\n",
    "                pl.col('value').alias('value_1'),\n",
    "                ((pl.col('value') - pl.col('value').mean().over('id')) / pl.col('value').std().over('id')).alias(\n",
    "                    'value_2'),\n",
    "                pl.col('value').cum_sum().over('id').alias('value_3'),\n",
    "                (pl.col('value').rank('dense').over('id') / pl.col('value').count().over('id')).alias('value_4'),\n",
    "                pl.col('value').abs().alias('value_5'),\n",
    "                pl.col('value').rolling_mean(16).over('id').alias('value_6'),\n",
    "                pl.col('value').rolling_std(16).over('id').alias('value_7'),\n",
    "            )\n",
    "            .with_columns(\n",
    "                pl.col('value_1').shift(1).over('id').alias('val_shift_1'),\n",
    "                pl.col('value_2').shift(1).over('id').alias('val_shift_2'),\n",
    "                pl.col('value_3').shift(1).over('id').alias('val_shift_3'),\n",
    "                pl.col('value_4').shift(1).over('id').alias('val_shift_4'),\n",
    "                pl.col('value_5').shift(1).over('id').alias('val_shift_5'),\n",
    "                pl.col('value_6').shift(1).over('id').alias('val_shift_6'),\n",
    "                pl.col('value_7').shift(1).over('id').alias('val_shift_7'),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return pl.concat([(\n",
    "            pl.concat([\n",
    "                X.filter(pl.col('period').eq(0)).group_by('id').tail(i),\n",
    "                X.filter(pl.col('period').eq(0)).group_by('id').tail(i * 2).with_columns(\n",
    "                    pl.lit(2).cast(pl.Int64).alias('period')),\n",
    "                X.filter(pl.col('period').eq(0)).group_by('id').tail(i * 3).with_columns(\n",
    "                    pl.lit(3).cast(pl.Int64).alias('period')),\n",
    "                X.filter(pl.col('period').eq(1)).group_by('id').head(i),\n",
    "            ])\n",
    "            .group_by('id', 'period')\n",
    "            .agg(flatten_2d([\n",
    "                                pl.corr(f'value_{j}', f'val_shift_{j}').alias(f'val_corr_{j}_{i}'),\n",
    "\n",
    "                                pl.col(f'value_{j}').quantile(0.25).alias(f'val_q25_{j}_{i}'),\n",
    "                                pl.col(f'value_{j}').quantile(0.50).alias(f'val_q50_{j}_{i}'),\n",
    "                                pl.col(f'value_{j}').quantile(0.75).alias(f'val_q75_{j}_{i}'),\n",
    "\n",
    "                                pl.mean(f'value_{j}').alias(f'val_mean_{j}_{i}'),\n",
    "                                pl.std(f'value_{j}').alias(f'val_std_{j}_{i}'),\n",
    "                                pl.min(f'value_{j}').alias(f'val_min_{j}_{i}'),\n",
    "                                pl.max(f'value_{j}').alias(f'val_max_{j}_{i}'),\n",
    "                                pl.col(f'value_{j}').skew().alias(f'val_skew_{j}_{i}'),\n",
    "                            ] for j in [1, 2, 3, 4, 5, 6, 7]))\n",
    "            .pivot(\n",
    "                index=['id'],\n",
    "                on=['period'],\n",
    "                values=flatten_2d([[f'val_mean_{j}_{i}', f'val_std_{j}_{i}', f'val_min_{j}_{i}', f'val_max_{j}_{i}',\n",
    "                                    f'val_skew_{j}_{i}', f'val_q25_{j}_{i}', f'val_q50_{j}_{i}', f'val_q75_{j}_{i}',\n",
    "                                    f'val_corr_{j}_{i}'] for j in [1, 2, 3, 4, 5, 6, 7]])\n",
    "            )\n",
    "            .with_columns(flatten_2d([\n",
    "                                         (pl.col(f'val_mean_{j}_{i}_0') - pl.col(f'val_mean_{j}_{i}_1')).alias(\n",
    "                                             f'val_mean_{j}_{i}_1_diff'),\n",
    "                                         (pl.col(f'val_std_{j}_{i}_0') - pl.col(f'val_std_{j}_{i}_1')).alias(\n",
    "                                             f'val_std_{j}_{i}_1_diff'),\n",
    "                                         (pl.col(f'val_min_{j}_{i}_0') - pl.col(f'val_min_{j}_{i}_1')).alias(\n",
    "                                             f'val_min_{j}_{i}_1_diff'),\n",
    "                                         (pl.col(f'val_max_{j}_{i}_0') - pl.col(f'val_max_{j}_{i}_1')).alias(\n",
    "                                             f'val_max_{j}_{i}_1_diff'),\n",
    "\n",
    "                                         (pl.col(f'val_q25_{j}_{i}_0') - pl.col(f'val_q25_{j}_{i}_1')).alias(\n",
    "                                             f'val_q25_{j}_{i}_1_diff'),\n",
    "                                         (pl.col(f'val_q50_{j}_{i}_0') - pl.col(f'val_q50_{j}_{i}_1')).alias(\n",
    "                                             f'val_q50_{j}_{i}_1_diff'),\n",
    "                                         (pl.col(f'val_q75_{j}_{i}_0') - pl.col(f'val_q75_{j}_{i}_1')).alias(\n",
    "                                             f'val_q75_{j}_{i}_1_diff'),\n",
    "\n",
    "                                         (pl.col(f'val_mean_{j}_{i}_2') - pl.col(f'val_mean_{j}_{i}_1')).alias(\n",
    "                                             f'val_mean_{j}_{i}_2_diff'),\n",
    "                                         (pl.col(f'val_std_{j}_{i}_2') - pl.col(f'val_std_{j}_{i}_1')).alias(\n",
    "                                             f'val_std_{j}_{i}_2_diff'),\n",
    "                                         (pl.col(f'val_min_{j}_{i}_2') - pl.col(f'val_min_{j}_{i}_1')).alias(\n",
    "                                             f'val_min_{j}_{i}_2_diff'),\n",
    "                                         (pl.col(f'val_max_{j}_{i}_2') - pl.col(f'val_max_{j}_{i}_1')).alias(\n",
    "                                             f'val_max_{j}_{i}_2_diff'),\n",
    "\n",
    "                                         (pl.col(f'val_q25_{j}_{i}_2') - pl.col(f'val_q25_{j}_{i}_1')).alias(\n",
    "                                             f'val_q25_{j}_{i}_2_diff'),\n",
    "                                         (pl.col(f'val_q50_{j}_{i}_2') - pl.col(f'val_q50_{j}_{i}_1')).alias(\n",
    "                                             f'val_q50_{j}_{i}_2_diff'),\n",
    "                                         (pl.col(f'val_q75_{j}_{i}_2') - pl.col(f'val_q75_{j}_{i}_1')).alias(\n",
    "                                             f'val_q75_{j}_{i}_2_diff'),\n",
    "\n",
    "                                         (pl.col(f'val_mean_{j}_{i}_3') - pl.col(f'val_mean_{j}_{i}_1')).alias(\n",
    "                                             f'val_mean_{j}_{i}_3_diff'),\n",
    "                                         (pl.col(f'val_std_{j}_{i}_3') - pl.col(f'val_std_{j}_{i}_1')).alias(\n",
    "                                             f'val_std_{j}_{i}_3_diff'),\n",
    "                                         (pl.col(f'val_min_{j}_{i}_3') - pl.col(f'val_min_{j}_{i}_1')).alias(\n",
    "                                             f'val_min_{j}_{i}_3_diff'),\n",
    "                                         (pl.col(f'val_max_{j}_{i}_3') - pl.col(f'val_max_{j}_{i}_1')).alias(\n",
    "                                             f'val_max_{j}_{i}_3_diff'),\n",
    "\n",
    "                                         (pl.col(f'val_q25_{j}_{i}_3') - pl.col(f'val_q25_{j}_{i}_1')).alias(\n",
    "                                             f'val_q25_{j}_{i}_3_diff'),\n",
    "                                         (pl.col(f'val_q50_{j}_{i}_3') - pl.col(f'val_q50_{j}_{i}_1')).alias(\n",
    "                                             f'val_q50_{j}_{i}_3_diff'),\n",
    "                                         (pl.col(f'val_q75_{j}_{i}_3') - pl.col(f'val_q75_{j}_{i}_1')).alias(\n",
    "                                             f'val_q75_{j}_{i}_3_diff'),\n",
    "\n",
    "                                         (pl.col(f'val_corr_{j}_{i}_0') - pl.col(f'val_corr_{j}_{i}_1')).alias(\n",
    "                                             f'val_corr_{j}_{i}_0_diff'),\n",
    "                                         (pl.col(f'val_corr_{j}_{i}_2') - pl.col(f'val_corr_{j}_{i}_1')).alias(\n",
    "                                             f'val_corr_{j}_{i}_2_diff'),\n",
    "                                         (pl.col(f'val_corr_{j}_{i}_3') - pl.col(f'val_corr_{j}_{i}_1')).alias(\n",
    "                                             f'val_corr_{j}_{i}_3_diff'),\n",
    "                                     ] for j in [1, 2, 3, 4, 5, 6, 7]))\n",
    "            .sort('id')\n",
    "            .drop('id')\n",
    "            .rename(lambda col: suffix + col)\n",
    "        ) for i in [20, 60, 120, 500, 1000]], how='horizontal')"
   ],
   "id": "4895807c4e7cb433",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T20:13:06.656612Z",
     "start_time": "2025-09-28T20:13:06.651866Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FourthFeatureGenerator:\n",
    "    def perform_hypothesis_test(self, df_group: pl.DataFrame) -> pl.DataFrame:\n",
    "        id = df_group['id'].to_numpy()[0]\n",
    "        a = df_group.filter(pl.col('period') == 0)['value'].to_numpy()\n",
    "        b = df_group.filter(pl.col('period') == 1)['value'].to_numpy()\n",
    "\n",
    "        n1, n2 = len(a), len(b)\n",
    "        s1, s2 = np.var(a, ddof=1), np.var(b, ddof=1)\n",
    "\n",
    "        f_statistic = s1 / s2\n",
    "        f_p_value = 2 * min(f.cdf(f_statistic, n1-1, n2-1), 1 - f.cdf(f_statistic, n1-1, n2-1))\n",
    "\n",
    "        levene_statistic, levene_p_value = levene(a, b)\n",
    "        ks_statistic, ks_p_value = ks_2samp(a, b)\n",
    "\n",
    "        return pl.DataFrame({\n",
    "            'id': [id],\n",
    "            'f_statistic': [f_statistic],\n",
    "            'f_p_value': [f_p_value],\n",
    "            'levene_statistic': [levene_statistic],\n",
    "            'levene_p_value': [levene_p_value],\n",
    "            'ks_statistic': [ks_statistic],\n",
    "            'ks_p_value': [ks_p_value],\n",
    "        })\n",
    "\n",
    "    def generate(self, X, suffix):\n",
    "        return (\n",
    "            X\n",
    "            .with_columns(pl.col('value').abs())\n",
    "            .group_by('id').map_groups(self.perform_hypothesis_test)\n",
    "            .sort('id')\n",
    "            .drop('id')\n",
    "            .rename(lambda col: suffix + col)\n",
    "        )"
   ],
   "id": "2642b51cae3af38f",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T20:13:06.741986Z",
     "start_time": "2025-09-28T20:13:06.737235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_features(X):\n",
    "    gen1 = FirstFeatureGenerator()\n",
    "    gen2 = SecondFeatureGenerator()\n",
    "    gen3 = ThirdFeatureGenerator()\n",
    "    gen4 = FourthFeatureGenerator()\n",
    "\n",
    "    X = X.reset_index(level=1)\n",
    "    ids = X.index.unique().tolist()\n",
    "    X = pl.from_pandas(X.reset_index()).with_columns(pl.col('value').cast(pl.Float32))\n",
    "\n",
    "    X1 = gen1.generate(X, 'step1')\n",
    "    X2 = gen3.generate(X, 'step2')\n",
    "    X3 = gen2.generate(X, 'step3')\n",
    "    X4 = gen4.generate(X, 'step4')\n",
    "\n",
    "    X1 = X1[sorted(X1.columns)]\n",
    "    X2 = X2[sorted(X2.columns)]\n",
    "    X3 = X3[sorted(X3.columns)]\n",
    "    X4 = X4[sorted(X4.columns)]\n",
    "\n",
    "    XS = pl.concat([X2, X3, X4], how='horizontal')\n",
    "\n",
    "    XS = XS.to_pandas()\n",
    "    X1 = X1.to_pandas()\n",
    "\n",
    "    XS.index = ids\n",
    "    X1.index = ids\n",
    "\n",
    "    return X1, XS"
   ],
   "id": "eb8fe6ac83e91b92",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T20:13:06.765138Z",
     "start_time": "2025-09-28T20:13:06.761053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_feature_importance(X, y):\n",
    "    model = lgb.LGBMClassifier(\n",
    "        n_estimators=750,\n",
    "        learning_rate=0.01,\n",
    "        colsample_bytree=0.3,\n",
    "        max_depth=8,\n",
    "        random_state=42,\n",
    "        verbosity=-1,\n",
    "    )\n",
    "    model.fit(X, y)\n",
    "\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X)\n",
    "    if isinstance(shap_values, list) and len(shap_values) == 2:\n",
    "        shap_values = shap_values[1]\n",
    "\n",
    "    shap_importance = np.abs(shap_values).mean(axis=0)\n",
    "    shap_importance = pd.Series(shap_importance, index=X.columns).sort_values()\n",
    "    shap_importance = shap_importance.index.tolist()\n",
    "\n",
    "    gain_importance = model.booster_.feature_importance(importance_type='gain')\n",
    "    gain_importance = pd.Series(gain_importance, index=X.columns).sort_values()\n",
    "    gain_importance = gain_importance.index.tolist()\n",
    "\n",
    "    return shap_importance, gain_importance"
   ],
   "id": "a6ca4fcebdccae6",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T20:13:06.797308Z",
     "start_time": "2025-09-28T20:13:06.792599Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_gbdt(seed):\n",
    "    return lgb.LGBMClassifier(\n",
    "        n_estimators=5000,\n",
    "        learning_rate=0.01,\n",
    "        colsample_bytree=0.2,\n",
    "        bagging_freq=4,\n",
    "        bagging_fraction=0.8,\n",
    "        max_depth=8,\n",
    "        random_state=seed,\n",
    "        verbosity=-1,\n",
    "    )"
   ],
   "id": "e57bdd1789b1a2ba",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T20:13:06.831366Z",
     "start_time": "2025-09-28T20:13:06.823099Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultiInputClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, columns1, columns2):\n",
    "        self.model1 = build_gbdt(12)\n",
    "        self.model2 = build_gbdt(22)\n",
    "        self.model3 = build_gbdt(32)\n",
    "        self.model4 = build_gbdt(42)\n",
    "\n",
    "        self.columns1 = columns1\n",
    "        self.columns2 = columns2\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model1_ = clone(self.model1)\n",
    "        self.model2_ = clone(self.model2)\n",
    "        self.model3_ = clone(self.model3)\n",
    "        self.model4_ = clone(self.model4)\n",
    "\n",
    "        self.model1_.fit(X[self.columns1[-200:]], y)\n",
    "        self.model2_.fit(X[self.columns2[-200:]], y)\n",
    "        self.model3_.fit(X[self.columns1[-500:]], y)\n",
    "        self.model4_.fit(X[self.columns2[-500:]], y)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        pred1 = self.model1_.predict(X[self.columns1[-200:]])\n",
    "        pred2 = self.model2_.predict(X[self.columns2[-200:]])\n",
    "        pred3 = self.model3_.predict(X[self.columns1[-500:]])\n",
    "        pred4 = self.model4_.predict(X[self.columns2[-500:]])\n",
    "\n",
    "        preds = np.vstack([pred1, pred2, pred3, pred4])\n",
    "\n",
    "        return np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=preds)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        proba1 = self.model1_.predict_proba(X[self.columns1[-200:]])\n",
    "        proba2 = self.model2_.predict_proba(X[self.columns2[-200:]])\n",
    "        proba3 = self.model3_.predict_proba(X[self.columns1[-500:]])\n",
    "        proba4 = self.model4_.predict_proba(X[self.columns2[-500:]])\n",
    "\n",
    "        return (proba1 + proba2 + proba3 + proba4) / 4"
   ],
   "id": "74f14dc96035f0c5",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T20:13:06.857636Z",
     "start_time": "2025-09-28T20:13:06.851290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    model_directory_path: str,\n",
    "):\n",
    "    X1, X2 = generate_features(X_train)\n",
    "\n",
    "    model_tabpfn = [\n",
    "        joblib.load(os.path.join(model_directory_path, 'model_tabpfn_0.joblib')),\n",
    "        joblib.load(os.path.join(model_directory_path, 'model_tabpfn_1.joblib')),\n",
    "        joblib.load(os.path.join(model_directory_path, 'model_tabpfn_2.joblib')),\n",
    "        joblib.load(os.path.join(model_directory_path, 'model_tabpfn_3.joblib')),\n",
    "        joblib.load(os.path.join(model_directory_path, 'model_tabpfn_4.joblib')),\n",
    "    ]\n",
    "\n",
    "    pred_stack = []\n",
    "    cv = KFold(5, shuffle=True, random_state=42)\n",
    "    for i, (idx_train, idx_valid) in enumerate(cv.split(X1, y_train)):\n",
    "        model_tabpfn[i].fit(X1.iloc[idx_train], y_train.iloc[idx_train])\n",
    "        joblib.dump(model_tabpfn[i], os.path.join(model_directory_path, f'model_tabpfn_{i}.joblib'))\n",
    "        pred = model_tabpfn[i].predict_proba(X1.iloc[idx_valid])[:, 1]\n",
    "        pred = pd.DataFrame([pred]).T\n",
    "        pred.index = y_train.iloc[idx_valid].index\n",
    "        pred_stack.append(pred)\n",
    "\n",
    "    X = pd.concat(pred_stack, axis=0).sort_index()\n",
    "    X = X.add_prefix('col_')\n",
    "    X = pd.concat([X2, X], axis=1)\n",
    "\n",
    "    shap_imp, gain_imp = get_feature_importance(X, y_train)\n",
    "\n",
    "    model_gbdt = MultiInputClassifier(\n",
    "        columns1=shap_imp, columns2=gain_imp,\n",
    "    )\n",
    "\n",
    "    model_gbdt.fit(X, y_train)\n",
    "    joblib.dump(model_gbdt, os.path.join(model_directory_path, 'model_gbdt.joblib'))"
   ],
   "id": "cd8893874ca3c540",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T20:13:06.878402Z",
     "start_time": "2025-09-28T20:13:06.873564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def infer(\n",
    "    X_test: typing.Iterable[pd.DataFrame],\n",
    "    model_directory_path: str,\n",
    "):\n",
    "    model_tabpfn = [\n",
    "        joblib.load(os.path.join(model_directory_path, 'model_tabpfn_0.joblib')),\n",
    "        joblib.load(os.path.join(model_directory_path, 'model_tabpfn_1.joblib')),\n",
    "        joblib.load(os.path.join(model_directory_path, 'model_tabpfn_2.joblib')),\n",
    "        joblib.load(os.path.join(model_directory_path, 'model_tabpfn_3.joblib')),\n",
    "        joblib.load(os.path.join(model_directory_path, 'model_tabpfn_4.joblib')),\n",
    "    ]\n",
    "    model_gbdt = joblib.load(os.path.join(model_directory_path, 'model_gbdt.joblib'))\n",
    "\n",
    "    yield  # Mark as ready\n",
    "\n",
    "    for dataset in X_test:\n",
    "        X1, X2 = generate_features(dataset)\n",
    "\n",
    "        X = pd.DataFrame([np.mean([model.predict_proba(X1)[:, 1] for model in model_tabpfn])]).T\n",
    "        X.index = X1.index\n",
    "        X = X.sort_index()\n",
    "        X = X.add_prefix('col_')\n",
    "        X = pd.concat([X2, X], axis=1)\n",
    "\n",
    "        prediction = model_gbdt.predict_proba(X)[:, 1]\n",
    "        yield prediction  # Send the prediction for the current dataset"
   ],
   "id": "1c77207fb433667",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T20:19:25.905337Z",
     "start_time": "2025-09-28T20:13:06.894102Z"
    }
   },
   "cell_type": "code",
   "source": [
    "crunch.test(\n",
    "    # Uncomment to disable the train\n",
    "    force_first_train=False,\n",
    "\n",
    "    # Uncomment to disable the determinism check\n",
    "    # no_determinism_check=True,\n",
    ")"
   ],
   "id": "4ab34cb632fd8d5f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m23:13:11\u001B[0m \u001B[33mno forbidden library found\u001B[0m\n",
      "\u001B[32m23:13:11\u001B[0m \u001B[33m\u001B[0m\n",
      "\u001B[32m23:13:12\u001B[0m started\n",
      "\u001B[32m23:13:12\u001B[0m running local test\n",
      "\u001B[32m23:13:12\u001B[0m \u001B[33minternet access isn't restricted, no check will be done\u001B[0m\n",
      "\u001B[32m23:13:12\u001B[0m \n",
      "\u001B[32m23:13:13\u001B[0m starting unstructured loop...\n",
      "\u001B[32m23:13:13\u001B[0m executing - command=train\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\X_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_train.parquet (204327238 bytes)\n",
      "data\\X_train.parquet: already exists, file length match\n",
      "data\\X_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_test.reduced.parquet (2380918 bytes)\n",
      "data\\X_test.reduced.parquet: already exists, file length match\n",
      "data\\y_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_train.parquet (61003 bytes)\n",
      "data\\y_train.parquet: already exists, file length match\n",
      "data\\y_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_test.reduced.parquet (2655 bytes)\n",
      "data\\y_test.reduced.parquet: already exists, file length match\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Farukcan\\PycharmProjects\\ADIA Lab Structural Break Challenge\\.venv\\Lib\\site-packages\\shap\\explainers\\_tree.py:583: UserWarning: LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "  warnings.warn(\n",
      "\u001B[32m23:17:50\u001B[0m executing - command=infer\n",
      "\u001B[32m23:19:03\u001B[0m checking determinism by executing the inference again with 30% of the data (tolerance: 1e-08)\n",
      "\u001B[32m23:19:03\u001B[0m executing - command=infer\n",
      "\u001B[32m23:19:25\u001B[0m determinism check: passed\n",
      "\u001B[32m23:19:25\u001B[0m \u001B[33msave prediction - path=data\\prediction.parquet\u001B[0m\n",
      "\u001B[32m23:19:25\u001B[0m ended\n",
      "\u001B[32m23:19:25\u001B[0m \u001B[33mduration - time=00:06:13\u001B[0m\n",
      "\u001B[32m23:19:25\u001B[0m \u001B[33mmemory - before=\"713.8 MB\" after=\"10.11 GB\" consumed=\"9.39 GB\"\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "source": "",
   "id": "a591f4fd547cf493",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
